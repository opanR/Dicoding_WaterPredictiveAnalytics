# -*- coding: utf-8 -*-
"""Predictive Analytic Potensi Aman Tidak Air Untuk Dikonsumsi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_bgQBx4srK2uLZU_Py3mFGcXyOC4TVS
"""

#Import Library
from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import  OneHotEncoder,StandardScaler
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor,ExtraTreesClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.naive_bayes import BernoulliNB

"""Upload file .csv air"""

files.upload()

# baca file air tersebut lalu tampilkan
df = pd.read_csv('/content/water_potability.csv')
df

# cek type data dari masing-masing kolom
df.info()

# cek informasik statistik dari masing-masing kolom
df.describe()

# Melakukan cek terhadap kolom mana saja yang memiliki value Null
columns_with_nan = df.columns[df.isnull().any()].tolist()

# Membuat DataFrame untuk menampilkan kolom yang memiliki nilai null beserta tipe datanya
nan_info = pd.DataFrame({
    'Tipe Data': df[columns_with_nan].dtypes,
    'Jumlah Null': df[columns_with_nan].isnull().sum()
})

print("Kolom yang memiliki nilai Null beserta tipe datanya:")
print(nan_info)

# Melakukan cek terhadap kolom mana saja yang memiliki value Null
columns_with_nan = df_cleaned.columns[df_cleaned.isnull().any()].tolist()

# Membuat DataFrame untuk menampilkan kolom yang memiliki nilai null beserta tipe datanya
nan_info = pd.DataFrame({
    'Tipe Data': df_cleaned[columns_with_nan].dtypes,
    'Jumlah Null': df_cleaned[columns_with_nan].isnull().sum()
})

print("Kolom yang memiliki nilai Null beserta tipe datanya:")
print(nan_info)
print("Jumlah akhir dataset: ",df_cleaned.shape)

#Mengcopy dataset agar dataset original tidak terpengaruhi
df_cleaned = df.copy()

# Melakukan drop terhadap kolom yang memiliki jumlah Null
df_cleaned = df_cleaned.dropna(subset=['ph', 'Sulfate','Trihalomethanes'])

# Cek jumlah baris dan kolom
df.shape

# Cek jumlah duplikat
df.duplicated().sum()

df.dropna(inplace=True)
df.isnull().sum().sum()

df.describe()

# Visualisasi Outlier
df_outlier=df.select_dtypes(exclude=['object'])
for column in df_outlier:
        plt.figure()
        sns.boxplot(data=df_outlier, x=column)

# Menghapus outliers yang ada pada dataset
for col in df.select_dtypes(include=['object']).columns:
    try:
        df[col] = pd.to_numeric(df[col])
    except ValueError:
        print(f"Could not convert column '{col}' to numeric. Check for non-numeric values. Consider dropping or using other encoding methods.")
        # Handle columns that couldn't be converted, e.g., drop them or use one-hot encoding
        # df = df.drop(columns=[col])  # Example: dropping the column

# Now proceed with the quantile calculation
# Select only numeric columns for quantile calculation
numeric_df = df.select_dtypes(include=['number'])
Q1 = numeric_df.quantile(0.25)
Q3 = numeric_df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((numeric_df < (Q1 - 1.5 * IQR)) | (numeric_df > (Q3 + 1.5 * IQR))).any(axis=1)]

# Cek Jumlah Datasets setalah kita hapus Outlier:
df.shape

# Lakukan EDA - Univariate Analysis
df.hist(bins=50, figsize=(20,15))
plt.show()

# Lakukan EDA - Multivariate Analysis
sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.3, )
plt.title(f"Matriks Korelasi Antar Kolo, ", size=30)

ax = sns.countplot(x="Potability", data=df, saturation=0.8)
plt.xticks(ticks=[0, 1], labels=["Tidak Bisa Diminum", "Bisa Diminum"])
plt.show()

x = df.drop("Potability",axis=1)
y = df.Potability

x.shape,y.shape

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=70)

scaler = MinMaxScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

!pip install lazypredict

from lazypredict.Supervised import LazyClassifier
clf = LazyClassifier()
models,predicts = clf.fit(x_train,x_test,y_train,y_test)
print(models.sort_values(by="Accuracy",ascending=False))

temp = models.sort_values(by="Accuracy",ascending=True)
plt.figure(figsize=(10, 8))
plt.barh(temp.index,temp["Accuracy"])
plt.show()

models = pd.DataFrame(index=['accuracy_score'],
                      columns=['KNN', 'RandomForest', 'SVM', 'Naive Bayes','Extra trees classifier'])

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Explore different values for k
    'weights': ['uniform', 'distance'],  # Explore different weight functions
    'metric': ['euclidean', 'manhattan']  # Explore different distance metrics
}

# Create a KNeighborsClassifier object
knn = KNeighborsClassifier()

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the GridSearchCV object to the training data
grid_search.fit(x_train, y_train)

# Print the best hyperparameters found
print("Best hyperparameters:", grid_search.best_params_)

# Get the best model
best_knn = grid_search.best_estimator_

# Evaluate the best model on the test data
knn_pred = best_knn.predict(x_test)
models.loc['accuracy_score', 'KNN'] = accuracy_score(y_test, knn_pred)

print(f"Accuracy of the best KNN model: {accuracy_score(y_test, knn_pred)}")

model_knn = KNeighborsClassifier(n_neighbors=2, weights='uniform', metric= 'euclidean')
model_knn.fit(x_train, y_train)

knn_pred = model_knn.predict(x_test)
models.loc['accuracy_score','KNN'] = accuracy_score(y_test, knn_pred)

model_rf = RandomForestClassifier(max_depth= 20)
model_rf.fit(x_train, y_train)

rf_pred = model_rf.predict(x_test)
models.loc['accuracy_score','RandomForest'] = accuracy_score(y_test, rf_pred)

model_svc = SVC()
model_svc.fit(x_train, y_train)

svc_pred = model_svc.predict(x_test)
models.loc['accuracy_score','SVM'] = accuracy_score(y_test, svc_pred)

model_nb = BernoulliNB()
model_nb.fit(x_train, y_train)

nb_pred = model_nb.predict(x_test)
models.loc['accuracy_score','Naive Bayes'] = accuracy_score(y_test, nb_pred)

model_etc = ExtraTreesClassifier(n_estimators=100, max_depth= 10,n_jobs= 2,random_state= 100)
model_etc.fit(x_train, y_train)

etc_pred = model_etc.predict(x_test)
models.loc['accuracy_score','Extra trees classifier'] = accuracy_score(y_test, etc_pred)

print(models)

plt.bar('KNN', models['KNN'])
plt.bar('RandomForest', models['RandomForest'])
plt.bar('SVM', models['SVM'])
plt.bar('Naive Bayes', models['Naive Bayes'])
plt.bar('Extra Trees', models['Extra trees classifier'])
plt.title("Perbandingan Akurasi Model");
plt.xlabel('Model');
plt.ylabel('Akurasi');
plt.show()